{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello world! I am He Zhang.\n"
     ]
    }
   ],
   "source": [
    "print(\"Hello world! I am He Zhang.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Task description:\n",
    "## Crawl Weibo information (texts and images) of a specific Weibo user.\n",
    "## Target Weibo user: Ju Jingyi (https://m.weibo.cn/u/3669102477)\n",
    "\n",
    "## Requirement: Python 3.0+.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from lxml import html\n",
    "import requests\n",
    "import json\n",
    "import re\n",
    "import os\n",
    "import time\n",
    "import urllib.request\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Set webpage request headers.\n",
    "\n",
    "headers = {\n",
    "'Cookie': 'XXXXXXXXXXXXXXX',\n",
    "'Host': 'm.weibo.cn',\n",
    "'Referer': 'https://m.weibo.cn/u/3669102477',\n",
    "'Upgrade-Insecure-Requests': '1',\n",
    "'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_13_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/71.0.3578.98 Safari/537.36',\n",
    "}\n",
    "\n",
    "# The information of \"headers\" can be obtained as below:\n",
    "# -> DevTools (F12) -> \"XHR\" -> Refresh webpage (F5)\n",
    "# -> select \"getindex?type...\" in \"Name\" -> \"Headers\"\n",
    "# -> \"Request Headers\" -> ... -> done.\n",
    "\n",
    "# Note: \n",
    "# The \"Cookie\" is obtained from https://m.weibo.cn/u/3669102477 (login required).\n",
    "# The \"Cookie\" might be changed in hours and should be updated accordingly!!!\n",
    "# The \"Referer\" should be changed for different Weibo user!!!\n",
    "\n",
    "user_url = 'https://m.weibo.cn/api/container/getIndex?type=uid&value=3669102477&containerid=1076033669102477'\n",
    "\n",
    "# The \"user_url\" can be obtained as below: \n",
    "# -> DevTools (F12) -> \"XHR\" -> Refresh webpage (F5)\n",
    "# -> select \"getindex?type...\" in \"Name\" -> \"Headers\"\n",
    "# -> \"General\" -> \"Request URL\" -> done.\n",
    "\n",
    "user_name = 'JuJingyi'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start crawling all cards on page 1 ...\n",
      "... Finished.\n",
      "Suspend 5 seconds.\n",
      "------------------------------\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "page = 1\n",
    "# page (int): Specified pages for crawling.\n",
    "\n",
    "# Note: \n",
    "# There are 13 posts on one page.\n",
    "# Set \"page\" = total posts/13.\n",
    "# For Ju Jingyi, \"page\" = 808/13.\n",
    "\n",
    "## Obtain the list of \"card\" information on specified pages.\n",
    "\n",
    "ii = 0  # Serial number of pages.\n",
    "list_cards = []  # Store \"card\" information.\n",
    "while ii < page:\n",
    "    ii = ii + 1\n",
    "    print('Start crawling all cards on page %d ...' % ii)\n",
    "    url = user_url + '&page=' + str(ii-1)  \n",
    "    # Note: The serial number of Weibo pages starts from 0!!!\n",
    "    \n",
    "    response = requests.get(url, headers=headers)\n",
    "    ob_json = json.loads(response.text)  \n",
    "    # ob_json (dict)\n",
    "    \n",
    "    list_cards.append(ob_json['data']['cards'])  \n",
    "    # ob_json['data']['cards'] (list)\n",
    "    \n",
    "    print('... Finished.')\n",
    "    \n",
    "    time.sleep(5)\n",
    "    print('Suspend 5 seconds.' + '\\n' + 30 * '-')\n",
    "    print('\\n')\n",
    "    # Suspend 5 seconds after crawling cards on one page.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "The number of pages for crawling is: 1.\n",
      "========================================\n",
      "\n",
      "\n",
      "\n",
      "Start crawling the 1-th card on 1-th page.\n",
      "Download the 1-th image of this weibo.\n",
      "Download the 2-th image of this weibo.\n",
      "Download the 3-th image of this weibo.\n",
      "... Finished.\n",
      "Suspend 5 seconds.\n",
      "\n",
      "... Finished.\n",
      "Suspend 5 seconds.\n",
      "\n",
      "\n",
      "\n",
      "Start crawling the 3-th card on 1-th page.\n",
      "... Finished.\n",
      "Suspend 5 seconds.\n",
      "\n",
      "\n",
      "\n",
      "Start crawling the 4-th card on 1-th page.\n",
      "Download the 1-th image of this weibo.\n",
      "Download the 2-th image of this weibo.\n",
      "Download the 3-th image of this weibo.\n",
      "Download the 4-th image of this weibo.\n",
      "... Finished.\n",
      "Suspend 5 seconds.\n",
      "\n",
      "... Finished.\n",
      "Suspend 5 seconds.\n",
      "\n",
      "\n",
      "\n",
      "Start crawling the 6-th card on 1-th page.\n",
      "Download the 1-th image of this weibo.\n",
      "Download the 2-th image of this weibo.\n",
      "Download the 3-th image of this weibo.\n",
      "... Finished.\n",
      "Suspend 5 seconds.\n",
      "\n",
      "\n",
      "\n",
      "Start crawling the 7-th card on 1-th page.\n",
      "Download the 1-th image of this weibo.\n",
      "Download the 2-th image of this weibo.\n",
      "Download the 3-th image of this weibo.\n",
      "Download the 4-th image of this weibo.\n",
      "Download the 5-th image of this weibo.\n",
      "Download the 6-th image of this weibo.\n",
      "Download the 7-th image of this weibo.\n",
      "Download the 8-th image of this weibo.\n",
      "Download the 9-th image of this weibo.\n",
      "... Finished.\n",
      "Suspend 5 seconds.\n",
      "\n",
      "\n",
      "\n",
      "Start crawling the 8-th card on 1-th page.\n",
      "... Finished.\n",
      "Suspend 5 seconds.\n",
      "\n",
      "\n",
      "\n",
      "Start crawling the 9-th card on 1-th page.\n",
      "Download the 1-th image of this weibo.\n",
      "Download the 2-th image of this weibo.\n",
      "... Finished.\n",
      "Suspend 5 seconds.\n",
      "\n",
      "\n",
      "\n",
      "Start crawling the 10-th card on 1-th page.\n",
      "... Finished.\n",
      "Suspend 5 seconds.\n",
      "\n",
      "\n",
      "\n",
      "Start crawling the 11-th card on 1-th page.\n",
      "Download the 1-th image of this weibo.\n",
      "... Finished.\n",
      "Suspend 5 seconds.\n",
      "\n",
      "... Finished.\n",
      "Suspend 5 seconds.\n",
      "\n",
      "\n",
      "\n",
      "Start crawling the 13-th card on 1-th page.\n",
      "Download the 1-th image of this weibo.\n",
      "... Finished.\n",
      "Suspend 5 seconds.\n",
      "\n",
      "... Completed.\n"
     ]
    }
   ],
   "source": [
    "## Crawl Weibo data (text and images).\n",
    "\n",
    "print(40 * '=' + '\\n' + 'The number of pages for crawling is: ' + str(len(list_cards)) + '.' + '\\n' + 40 * '=' + '\\n')\n",
    "\n",
    "count_weibo = 1  # Serial number of cards.\n",
    "page_weibo = 1  # Serial number of pages.\n",
    "\n",
    "path = user_name + '_Weibo/'\n",
    "# Path for saving all Weibo information.\n",
    "# Note: The path should be empty before crawling!!!\n",
    "\n",
    "for cards in list_cards:\n",
    "    for card in cards:\n",
    "        if card['card_type'] == 9:\n",
    "            \n",
    "            print('\\n')\n",
    "            print('Start crawling the ' + str(count_weibo) + '-th card on ' + str(page_weibo) + '-th page.')\n",
    "            \n",
    "            mid = card['mblog']['id']\n",
    "            created_at = card['mblog']['created_at']\n",
    "            \n",
    "            # Crawl Weibo posts information.\n",
    "            if card['mblog']['isLongText'] == 'false':\n",
    "                text = card['mblog']['text']\n",
    "            else:\n",
    "                url = 'https://m.weibo.cn/statuses/extend?id=' + mid\n",
    "                response = requests.get(url, headers=headers)\n",
    "                ob_json = json.loads(response.text)  # ob_json (dict)\n",
    "                text = ob_json['data']['longTextContent']\n",
    "                tree = html.fromstring(text)\n",
    "                text = tree.xpath('string(.)')\n",
    "            \n",
    "            # Save text of posts.\n",
    "            with open(path + 'weibo_crawl.txt', 'a', encoding='utf-8') as ff:\n",
    "                ff.write('\\n' + 'The ' + str(count_weibo) + '-th weibo\\n' + '***  Published on  ' + created_at + '  ***' + '\\n')\n",
    "                ff.write(text + '\\n')\n",
    "            \n",
    "            # Save imgae of posts.\n",
    "            if 'bmiddle_pic' in card['mblog']:\n",
    "                image_path = path + str(count_weibo)\n",
    "                os.mkdir(image_path)\n",
    "                url_extend = 'https://m.weibo.cn/status/' + mid  # URL of one Weibo.\n",
    "                res = requests.get(url_extend, headers=headers).text  # (string)\n",
    "                imgurl_weibo = re.findall('https://.*large.*.jpg', res)  # Match URL of image.\n",
    "                x = 1\n",
    "                for i in range(len(imgurl_weibo)):\n",
    "                    temp = image_path + '/' + str(x) + '.jpg'\n",
    "                    # Add image URL to text file.\n",
    "                    with open(path + 'weibo_crawl.txt', 'a', encoding='utf-8') as ff:\n",
    "                        ff.write('The link of image is：' + imgurl_weibo[i] + '\\n')\n",
    "                    print('Download the %s-th image of this weibo.' % x)\n",
    "                    try:\n",
    "                        urllib.request.urlretrieve(urllib.request.urlopen(imgurl_weibo[i]).geturl(), temp)\n",
    "                    except:\n",
    "                        print(\"Failed to download %s-th image.\" % imgurl_weibo)\n",
    "                    x += 1\n",
    "        \n",
    "        count_weibo = count_weibo + 1\n",
    "        print('... Finished.')\n",
    "        \n",
    "        time.sleep(5)\n",
    "        print('Suspend 5 seconds.\\n')  \n",
    "        # Suspend 5 seconds after crawling one Weibo post.\n",
    "    \n",
    "    page_weibo = page_weibo + 1\n",
    "    print('... Completed.')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
