{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello world! I am He Zhang.\n"
     ]
    }
   ],
   "source": [
    "print(\"Hello world! I am He Zhang.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Task description:\n",
    "## Crawl Weibo information (posted text and images) of one specific Weibo user.\n",
    "## Example: Ju Jingyi (https://m.weibo.cn/u/3669102477).\n",
    "\n",
    "## Requirement: Python 3.0+.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from lxml import html\n",
    "import requests\n",
    "import json\n",
    "import re\n",
    "import os\n",
    "import time\n",
    "import urllib.request\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Set webpage request headers.\n",
    "\n",
    "headers = {\n",
    "'Cookie': 'XXXXXXXXXXXXXXX',\n",
    "'Host': 'm.weibo.cn',\n",
    "'Referer': 'https://m.weibo.cn/u/3669102477',\n",
    "'Upgrade-Insecure-Requests': '1',\n",
    "'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_13_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/71.0.3578.98 Safari/537.36',\n",
    "}\n",
    "\n",
    "# The information of \"headers\" can be obtained as below:\n",
    "# -> DevTools (F12) -> \"XHR\" -> Refresh webpage (F5)\n",
    "# -> select \"getindex?type...\" in \"Name\" -> \"Headers\"\n",
    "# -> \"Request Headers\" -> ... -> done.\n",
    "\n",
    "# Note: \n",
    "# The \"Cookie\" is obtained from https://m.weibo.cn/u/3669102477 (login required).\n",
    "# The \"Cookie\" might be changed in (?) hours and should be updated accordingly!!!\n",
    "# The \"Referer\" should be changed for different Weibo user!!!\n",
    "\n",
    "# Question: \n",
    "# By setting 'Cookie': \"XXX...XXX\", the code can still crawl Weibo data??? \n",
    "\n",
    "user_url = 'https://m.weibo.cn/api/container/getIndex?type=uid&value=3669102477&containerid=1076033669102477'\n",
    "\n",
    "# The \"user_url\" can be obtained as below: \n",
    "# -> DevTools (F12) -> \"XHR\" -> Refresh webpage (F5)\n",
    "# -> select \"getindex?type...\" in \"Name\" -> \"Headers\"\n",
    "# -> \"General\" -> \"Request URL\" -> done.\n",
    "\n",
    "user_name = 'JuJingyi'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start crawling all \"cards\" on page 1.\n",
      "Finish crawling all \"cards\" on page 1.\n",
      "Suspend 2 seconds.\n",
      "------------------------------\n",
      "\n",
      "Start crawling all \"cards\" on page 2.\n",
      "Finish crawling all \"cards\" on page 2.\n",
      "Suspend 2 seconds.\n",
      "------------------------------\n",
      "\n",
      "Start crawling all \"cards\" on page 3.\n",
      "Finish crawling all \"cards\" on page 3.\n",
      "Suspend 2 seconds.\n",
      "------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "page = 3\n",
    "# page (int): Total pages for crawling.\n",
    "\n",
    "# Note: \n",
    "# There are 11` posts on one page.\n",
    "# Set \"page\" = total posts/11.\n",
    "# Example for Ju Jingyi, \"page\" = 808`/11 = 74.\n",
    "\n",
    "## Obtain the list of \"cards\" information on specified pages.\n",
    "\n",
    "ii = 0  # Serial number of pages.\n",
    "list_cards = []  # Store \"cards\" information.\n",
    "while ii < page:\n",
    "    ii = ii + 1\n",
    "    print('Start crawling all \"cards\" on page %d.' % ii)\n",
    "    url = user_url + '&page=' + str(ii)  \n",
    "    \n",
    "    response = requests.get(url, headers=headers)\n",
    "    ob_json = json.loads(response.text)  \n",
    "    # ob_json (dict)\n",
    "    \n",
    "    list_cards.append(ob_json['data']['cards'])  \n",
    "    # ob_json['data']['cards'] (list)\n",
    "    \n",
    "    print('Finish crawling all \"cards\" on page ' + str(ii) + '.')\n",
    "    \n",
    "    print('Suspend 2 seconds.' + '\\n' + 30 * '-' + '\\n')\n",
    "    time.sleep(2)\n",
    "    # Suspend 2 seconds after crawling cards on one page.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "The number of pages for crawling is: 3.\n",
      "========================================\n",
      "\n",
      "Start crawling the 1-th post on 1-th page.\n",
      "Download the 1-th image of this weibo.\n",
      "Download the 2-th image of this weibo.\n",
      "Download the 3-th image of this weibo.\n",
      "Finish crawling the 1-th Weibo post.\n",
      "Suspend 2 seconds.\n",
      "\n",
      "Start crawling the 2-th post on 1-th page.\n",
      "Finish crawling the 2-th Weibo post.\n",
      "Suspend 2 seconds.\n",
      "\n",
      "Start crawling the 3-th post on 1-th page.\n",
      "Finish crawling the 3-th Weibo post.\n",
      "Suspend 2 seconds.\n",
      "\n",
      "Start crawling the 4-th post on 1-th page.\n",
      "Download the 1-th image of this weibo.\n",
      "Download the 2-th image of this weibo.\n",
      "Download the 3-th image of this weibo.\n",
      "Download the 4-th image of this weibo.\n",
      "Finish crawling the 4-th Weibo post.\n",
      "Suspend 2 seconds.\n",
      "\n",
      "Start crawling the 5-th post on 1-th page.\n",
      "Download the 1-th image of this weibo.\n",
      "Download the 2-th image of this weibo.\n",
      "Download the 3-th image of this weibo.\n",
      "Finish crawling the 5-th Weibo post.\n",
      "Suspend 2 seconds.\n",
      "\n",
      "Start crawling the 6-th post on 1-th page.\n",
      "Download the 1-th image of this weibo.\n",
      "Download the 2-th image of this weibo.\n",
      "Download the 3-th image of this weibo.\n",
      "Download the 4-th image of this weibo.\n",
      "Download the 5-th image of this weibo.\n",
      "Download the 6-th image of this weibo.\n",
      "Download the 7-th image of this weibo.\n",
      "Download the 8-th image of this weibo.\n",
      "Download the 9-th image of this weibo.\n",
      "Finish crawling the 6-th Weibo post.\n",
      "Suspend 2 seconds.\n",
      "\n",
      "Start crawling the 7-th post on 1-th page.\n",
      "Finish crawling the 7-th Weibo post.\n",
      "Suspend 2 seconds.\n",
      "\n",
      "Start crawling the 8-th post on 1-th page.\n",
      "Download the 1-th image of this weibo.\n",
      "Download the 2-th image of this weibo.\n",
      "Finish crawling the 8-th Weibo post.\n",
      "Suspend 2 seconds.\n",
      "\n",
      "Start crawling the 9-th post on 1-th page.\n",
      "Finish crawling the 9-th Weibo post.\n",
      "Suspend 2 seconds.\n",
      "\n",
      "Start crawling the 10-th post on 1-th page.\n",
      "Download the 1-th image of this weibo.\n",
      "Finish crawling the 10-th Weibo post.\n",
      "Suspend 2 seconds.\n",
      "\n",
      "Start crawling the 11-th post on 1-th page.\n",
      "Download the 1-th image of this weibo.\n",
      "Finish crawling the 11-th Weibo post.\n",
      "Suspend 2 seconds.\n",
      "\n",
      "Finish crawling Weibo data on 1-th page.\n",
      "----------------------------------------\n",
      "\n",
      "Start crawling the 12-th post on 2-th page.\n",
      "Finish crawling the 12-th Weibo post.\n",
      "Suspend 2 seconds.\n",
      "\n",
      "Start crawling the 13-th post on 2-th page.\n",
      "Download the 1-th image of this weibo.\n",
      "Download the 2-th image of this weibo.\n",
      "Download the 3-th image of this weibo.\n",
      "Download the 4-th image of this weibo.\n",
      "Download the 5-th image of this weibo.\n",
      "Download the 6-th image of this weibo.\n",
      "Finish crawling the 13-th Weibo post.\n",
      "Suspend 2 seconds.\n",
      "\n",
      "Start crawling the 14-th post on 2-th page.\n",
      "Download the 1-th image of this weibo.\n",
      "Download the 2-th image of this weibo.\n",
      "Finish crawling the 14-th Weibo post.\n",
      "Suspend 2 seconds.\n",
      "\n",
      "Start crawling the 15-th post on 2-th page.\n",
      "Download the 1-th image of this weibo.\n",
      "Download the 2-th image of this weibo.\n",
      "Download the 3-th image of this weibo.\n",
      "Finish crawling the 15-th Weibo post.\n",
      "Suspend 2 seconds.\n",
      "\n",
      "Start crawling the 16-th post on 2-th page.\n",
      "Download the 1-th image of this weibo.\n",
      "Download the 2-th image of this weibo.\n",
      "Finish crawling the 16-th Weibo post.\n",
      "Suspend 2 seconds.\n",
      "\n",
      "Start crawling the 17-th post on 2-th page.\n",
      "Download the 1-th image of this weibo.\n",
      "Download the 2-th image of this weibo.\n",
      "Download the 3-th image of this weibo.\n",
      "Download the 4-th image of this weibo.\n",
      "Finish crawling the 17-th Weibo post.\n",
      "Suspend 2 seconds.\n",
      "\n",
      "Start crawling the 18-th post on 2-th page.\n",
      "Finish crawling the 18-th Weibo post.\n",
      "Suspend 2 seconds.\n",
      "\n",
      "Start crawling the 19-th post on 2-th page.\n",
      "Finish crawling the 19-th Weibo post.\n",
      "Suspend 2 seconds.\n",
      "\n",
      "Start crawling the 20-th post on 2-th page.\n",
      "Finish crawling the 20-th Weibo post.\n",
      "Suspend 2 seconds.\n",
      "\n",
      "Start crawling the 21-th post on 2-th page.\n",
      "Download the 1-th image of this weibo.\n",
      "Download the 2-th image of this weibo.\n",
      "Finish crawling the 21-th Weibo post.\n",
      "Suspend 2 seconds.\n",
      "\n",
      "Finish crawling Weibo data on 2-th page.\n",
      "----------------------------------------\n",
      "\n",
      "Start crawling the 22-th post on 3-th page.\n",
      "Download the 1-th image of this weibo.\n",
      "Download the 2-th image of this weibo.\n",
      "Download the 3-th image of this weibo.\n",
      "Finish crawling the 22-th Weibo post.\n",
      "Suspend 2 seconds.\n",
      "\n",
      "Start crawling the 23-th post on 3-th page.\n",
      "Download the 1-th image of this weibo.\n",
      "Download the 2-th image of this weibo.\n",
      "Download the 3-th image of this weibo.\n",
      "Download the 4-th image of this weibo.\n",
      "Download the 5-th image of this weibo.\n",
      "Download the 6-th image of this weibo.\n",
      "Download the 7-th image of this weibo.\n",
      "Download the 8-th image of this weibo.\n",
      "Download the 9-th image of this weibo.\n",
      "Finish crawling the 23-th Weibo post.\n",
      "Suspend 2 seconds.\n",
      "\n",
      "Start crawling the 24-th post on 3-th page.\n",
      "Finish crawling the 24-th Weibo post.\n",
      "Suspend 2 seconds.\n",
      "\n",
      "Start crawling the 25-th post on 3-th page.\n",
      "Finish crawling the 25-th Weibo post.\n",
      "Suspend 2 seconds.\n",
      "\n",
      "Start crawling the 26-th post on 3-th page.\n",
      "Download the 1-th image of this weibo.\n",
      "Download the 2-th image of this weibo.\n",
      "Download the 3-th image of this weibo.\n",
      "Download the 4-th image of this weibo.\n",
      "Finish crawling the 26-th Weibo post.\n",
      "Suspend 2 seconds.\n",
      "\n",
      "Start crawling the 27-th post on 3-th page.\n",
      "Finish crawling the 27-th Weibo post.\n",
      "Suspend 2 seconds.\n",
      "\n",
      "Start crawling the 28-th post on 3-th page.\n",
      "Download the 1-th image of this weibo.\n",
      "Download the 2-th image of this weibo.\n",
      "Download the 3-th image of this weibo.\n",
      "Download the 4-th image of this weibo.\n",
      "Download the 5-th image of this weibo.\n",
      "Download the 6-th image of this weibo.\n",
      "Finish crawling the 28-th Weibo post.\n",
      "Suspend 2 seconds.\n",
      "\n",
      "Start crawling the 29-th post on 3-th page.\n",
      "Download the 1-th image of this weibo.\n",
      "Download the 2-th image of this weibo.\n",
      "Finish crawling the 29-th Weibo post.\n",
      "Suspend 2 seconds.\n",
      "\n",
      "Start crawling the 30-th post on 3-th page.\n",
      "Finish crawling the 30-th Weibo post.\n",
      "Suspend 2 seconds.\n",
      "\n",
      "Start crawling the 31-th post on 3-th page.\n",
      "Download the 1-th image of this weibo.\n",
      "Download the 2-th image of this weibo.\n",
      "Finish crawling the 31-th Weibo post.\n",
      "Suspend 2 seconds.\n",
      "\n",
      "Finish crawling Weibo data on 3-th page.\n",
      "----------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Crawl Weibo data (posted text and images).\n",
    "\n",
    "print(40 * '=' + '\\n' + 'The number of pages for crawling is: ' + str(len(list_cards)) + '.' + '\\n' + 40 * '=' + '\\n')\n",
    "\n",
    "count_weibo = 1  # Serial number of cards.\n",
    "page_weibo = 1  # Serial number of pages.\n",
    "\n",
    "path = user_name + '_Weibo/'\n",
    "# Path for saving all Weibo information.\n",
    "# Note: The path should be empty before crawling!!!\n",
    "\n",
    "for cards in list_cards:\n",
    "    for card in cards:\n",
    "        \n",
    "        print('Start crawling the ' + str(count_weibo) + '-th post on ' + str(page_weibo) + '-th page.')\n",
    "        \n",
    "        if card['card_type'] == 9:\n",
    "            mid = card['mblog']['id']\n",
    "            created_at = card['mblog']['created_at']\n",
    "            \n",
    "            # Crawl Weibo posts information.\n",
    "            if card['mblog']['isLongText'] == 'false':\n",
    "                text = card['mblog']['text']\n",
    "            else:\n",
    "                url = 'https://m.weibo.cn/statuses/extend?id=' + mid\n",
    "                response = requests.get(url, headers=headers)\n",
    "                ob_json = json.loads(response.text)  # ob_json (dict)\n",
    "                text = ob_json['data']['longTextContent']\n",
    "                tree = html.fromstring(text)\n",
    "                text = tree.xpath('string(.)')\n",
    "#             else:\n",
    "#                 try:\n",
    "#                     tmp_url = 'https://m.weibo.cn/statuses/extend?id=' + mid\n",
    "#                     tmp_response = requests.get(tmp_url, headers=headers)\n",
    "#                     ob_json = json.loads(tmp_response.text)  # ob_json (dict)\n",
    "#                     text = ob_json['data']['longTextContent']\n",
    "#                     tree = html.fromstring(text)\n",
    "#                     text = tree.xpath('string(.)')\n",
    "#                 except:\n",
    "#                     text = \"No short text extracted!\"\n",
    "            \n",
    "            # Save text of posts.\n",
    "            with open(path + 'weibo_crawl.txt', 'a', encoding='utf-8') as ff:\n",
    "                ff.write('\\n' + 'The ' + str(count_weibo) + '-th weibo\\n' + '***  Published on  ' + created_at + '  ***' + '\\n')\n",
    "                ff.write(text + '\\n')\n",
    "            \n",
    "            # Save imgae of posts.\n",
    "            if 'bmiddle_pic' in card['mblog']:\n",
    "                image_path = path + str(count_weibo)\n",
    "                os.mkdir(image_path)\n",
    "                url_extend = 'https://m.weibo.cn/status/' + mid  # URL of one Weibo.\n",
    "                res = requests.get(url_extend, headers=headers).text  # (string)\n",
    "                imgurl_weibo = re.findall('https://.*large.*.jpg', res)  # Match URL of image.\n",
    "                x = 1\n",
    "                for i in range(len(imgurl_weibo)):\n",
    "                    temp = image_path + '/' + str(x) + '.jpg'\n",
    "                    # Add image URL to text file.\n",
    "                    with open(path + 'weibo_crawl.txt', 'a', encoding='utf-8') as ff:\n",
    "                        ff.write('The link of image is：' + imgurl_weibo[i] + '\\n')\n",
    "                    print('Download the %s-th image of this weibo.' % x)\n",
    "                    try:\n",
    "                        urllib.request.urlretrieve(urllib.request.urlopen(imgurl_weibo[i]).geturl(), temp)\n",
    "                    except:\n",
    "                        print(\"Failed to download %s-th image.\" % imgurl_weibo[i])\n",
    "                    x += 1\n",
    "        \n",
    "        print('Finish crawling the ' + str(count_weibo) + '-th Weibo post.')\n",
    "        count_weibo = count_weibo + 1\n",
    "        \n",
    "        print('Suspend 2 seconds.\\n')\n",
    "        time.sleep(2)\n",
    "        # Suspend 2 seconds after crawling one Weibo post.\n",
    "    \n",
    "    print('Finish crawling Weibo data on ' + str(page_weibo) + '-th page.' + '\\n' + 40 * '-' + '\\n')\n",
    "    page_weibo = page_weibo + 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
